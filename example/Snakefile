"""
Generates kmers for samples
"""

configfile: "config_kmers.yaml"

ruleorder: count_kmers > filter_kmers

target_kmers = [21, 25, 31]

rule all:
    input:
        # Kmers
        expand("output/{sample}_k{thisk}.kmers", sample=config["sample"], thisk=target_kmers),
        expand("output/{sample}_CAGT_k{thisk}.kmers", sample=config["sample"], thisk=target_kmers),
        # SNPQuest Outputs
        expand("output_snp/{sample}_k{thisk}.vcf.gz", sample=config["sample"], thisk=target_kmers),
        expand("output_snp/{sample}_k{thisk}.vcf.gz.tbi", sample=config["sample"], thisk=target_kmers),
        # Merged VCFs
        expand("output_snp/Physalia_merged_k{thisk}.vcf", thisk=target_kmers),
        # ANGSD Outputs
        expand("output_snp/pca_output_k{thisk}.mafs", thisk=target_kmers),
        # Reference FASTA
        "output_snp/Physalia.pseudogenome.fasta",
        "output_snp/Physalia.pseudogenome.fasta.fai"


rule decompress_reads:
    input:
        reads = lambda wildcards: [f"{config['base_read_dir']}{x}" for x in config["sample"][wildcards.sample]["reads"]]
    output:
        R1 = temp("reads/{sample}_all_R1.fastq"),
        R2 = temp("reads/{sample}_all_R2.fastq")
    group:
        lambda wildcards: f"sample_{wildcards.sample}"
    params:
        sample = lambda wildcards: wildcards.sample,
        max_lines = 400000000
    shell:
        """
        # Concatenate reads
        set +o pipefail  # Disable strict mode so that pipe to head doesn't die
        zcat {input.reads}/*R1*.fastq.gz | head -n {params.max_lines} > {output.R1}
        zcat {input.reads}/*R2*.fastq.gz | head -n {params.max_lines} > {output.R2}
        """
        
        
rule trim_reads:
    input:
        R1 = "reads/{sample}_all_R1.fastq",
        R2 = "reads/{sample}_all_R2.fastq"
    output:
        R1_paired   = "reads/{sample}_trimmed_paired_R1.fastq",
        R1_unpaired = temp("reads/{sample}_trimmed_unpaired_R1.fastq"),
        R2_paired   = "reads/{sample}_trimmed_paired_R2.fastq",
        R2_unpaired = temp("reads/{sample}_trimmed_unpaired_R2.fastq")
    group:
        lambda wildcards: f"sample_{wildcards.sample}"
    params:
        sample = lambda wildcards: wildcards.sample
    threads: workflow.cores
    log: "logs/trimmomatic/trimmomatic_{sample}.log"
    shell:
        """
        # Trim reads
        echo ">PrefixPE/1\nTACACTCTTTCCCTACACGACGCTCTTCCGATCT\n>PrefixPE/2\nGTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT" > TruSeq3-PE.fa
        trimmomatic PE \
          -threads {threads} \
          {input.R1} \
          {input.R2} \
          {output.R1_paired} \
          {output.R1_unpaired} \
          {output.R2_paired} \
          {output.R2_unpaired} \
          ILLUMINACLIP:TruSeq3-PE.fa:2:30:10:2:True LEADING:15 TRAILING:15 MINLEN:50 2> {log}
        """

rule count_reads:
    input:
        R1 = "reads/{sample}_all_R1.fastq",
        R2 = "reads/{sample}_all_R2.fastq",
        R1_paired   = "reads/{sample}_trimmed_paired_R1.fastq",
        R1_unpaired = "reads/{sample}_trimmed_unpaired_R1.fastq",
        R2_paired   = "reads/{sample}_trimmed_paired_R2.fastq",
        R2_unpaired = "reads/{sample}_trimmed_unpaired_R2.fastq"
    output:
        read_count = "output/{sample}.count"
    group:
        lambda wildcards: f"sample_{wildcards.sample}"
    params:
        sample = lambda wildcards: wildcards.sample,
    shell:
        """
        # Count reads
        sum=0
        num_lines_raw=$(cat {input.R1} | wc -l)
        num_reads_raw=$(($num_lines_raw / 4))
        num_nucleotides_raw=$(cat {input.R1} {input.R2} | awk '{{if(NR%4==2) sum+=length($0)}} END {{print sum}}' )

        sum=0
        num_lines_trimmed=$(cat {input.R1_paired} | wc -l)
        num_reads_trimmed=$(($num_lines_trimmed / 4))
        num_nucleotides_trimmed=$(cat {input.R1_paired} {input.R2_paired} | awk '{{if(NR%4==2) sum+=length($0)}} END {{print sum}}' )

        sum=0
        num_lines_unpaired_trimmed=$(cat {input.R1_unpaired} {input.R2_unpaired} | wc -l)
        num_reads_unpaired_trimmed=$(($num_lines_unpaired_trimmed / 4))
        num_nucleotides_unpaired_trimmed=$(cat {input.R1_unpaired} {input.R2_unpaired} | awk '{{if(NR%4==2) sum+=length($0)}} END {{print sum}}' )

        echo "reads_pairs_raw\t$num_reads_raw" > {output.read_count}
        echo "nucleotides_raw\t$num_nucleotides_raw" >> {output.read_count}
        echo "reads_pairs_trimmed\t$num_reads_trimmed" >> {output.read_count}
        echo "nucleotides_trimmed\t$num_nucleotides_trimmed" >> {output.read_count}
        echo "reads_unpaired_trimmed\t$num_reads_unpaired_trimmed" >> {output.read_count}
        echo "nucleotides_unpaired_trimmed\t$num_nucleotides_unpaired_trimmed" >> {output.read_count}
        """


rule count_kmers:
    input:
        R1_paired   = "reads/{sample}_trimmed_paired_R1.fastq",
        R2_paired   = "reads/{sample}_trimmed_paired_R2.fastq"
    output:
        kmers = "output/{sample}_k{thisk}.kmers",
        jf = temporary("output/{sample}_k{thisk}.jf")
    group:
        lambda wildcards: f"sample_{wildcards.sample}"
    params:
        sample = lambda wildcards: wildcards.sample,
        thisk = lambda wildcards: wildcards.thisk,
    threads: workflow.cores
    log: "logs/jellyfish/jellyfish_{sample}_k{thisk}.log"
    shell:
        """
        jellyfish count -C -m {params.thisk} -s 1000000000 -t {threads} \
          -o {output.jf} \
          {input.R1_paired} {input.R2_paired} \
          2> {log}

        jellyfish dump -c -L 2 -o {output.kmers} {output.jf} 
        """

rule filter_kmers:
    input:
        kmers = "output/{sample}_k{thisk}.kmers"
    output:
        kmers = "output/{sample}_CAGT_k{thisk}.kmers"
    shell:
        """
        # Don't filter on first site. Since these are canonical kmers, that would remove some nucleotides from last position.
        grep "^.CAGT" {input.kmers} > {output.kmers}
        """

rule run_snpquest:
    input:
        kmers = expand("output/{sample}_CAGT_k{thisk}.kmers", sample=config["sample"], thisk="{thisk}")
    output:
        vcf = "output_snp/Physalia_k{thisk}.vcf",  # One VCF per k value
        fasta = "output_snp/Physalia.pseudogenome.fasta"
    log:
        "logs/snpquest/snpquest_k{thisk}.log"
    benchmark:
        "benchmarks/snpquest/snpquest_k{thisk}.txt"
    params:
        thisk = lambda wildcards: wildcards.thisk
    shell:
        """
        echo "[INFO $(date)] Starting SNPQuest with k={params.thisk}" >> {log}
        ../target/release/snpquest --outdir output_snp/ --freq-min 1.0 --run-name Physalia_k{params.thisk} {input} >> {log} 2>&1
        echo "[INFO $(date)] Completed SNPQuest with k={params.thisk}" >> {log}
        """



rule bgzip_vcf:
    input:
        "output_snp/{sample}_k{thisk}.vcf"
    output:
        "output_snp/{sample}_k{thisk}.vcf.gz"
    shell:
        "bgzip {input}"

rule index_vcf:
    input:
        "output_snp/{sample}_k{thisk}.vcf.gz"
    output:
        "output_snp/{sample}_k{thisk}.vcf.gz.tbi"
    shell:
        "tabix -p vcf {input}"

rule index_fasta:
    input:
        "output_snp/Physalia.pseudogenome.fasta"
    output:
        "output_snp/Physalia.pseudogenome.fasta.fai"
    shell:
        "samtools faidx {input}"

rule merge_vcf:
    input:
        tbi = expand("output_snp/{sample}_k{thisk}.vcf.gz.tbi", sample = config["sample"], thisk = target_kmers),
        vcf = expand("output_snp/{sample}_k{thisk}.vcf.gz", sample = config["sample"], thisk = target_kmers)
    output:
        "output_snp/Physalia_merged_k{thisk}.vcf"
    shell:
        "bcftools merge {input.vcf} -o {output}"

rule run_angsd:
    input:
        vcf = "output_snp/Physalia_merged_k{thisk}.vcf"
    output:
        "output_snp/pca_output_k{thisk}.mafs"
    params:
        thisk = lambda wildcards: wildcards.thisk
    log:
        "logs/angsd/angsd_{thisk}.log"
    shell:
        """
        angsd \
          -vcf-gl {input.vcf} \
          -doMajorMinor 1 \
          -doMaf 1 \
          -out output_snp/pca_output_k{params.thisk} >> {log} 2>&1
        """